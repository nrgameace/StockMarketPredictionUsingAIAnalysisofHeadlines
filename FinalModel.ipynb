{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-iA1dfttz79"
   },
   "outputs": [],
   "source": [
    "#Import required Python libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKbMfSc1uLpF"
   },
   "outputs": [],
   "source": [
    "#Imports each dataset from a file into Random Access Memory\n",
    "dfNews = pd.read_csv('/home/nickregas/Desktop/IndependentCompsciProject/Data/All_external.csv')\n",
    "dfStock = pd.read_csv('/home/nickregas/Desktop/IndependentCompsciProject/Data/VTI.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xdTt5WtuNVZ"
   },
   "outputs": [],
   "source": [
    "#Finds the Daily Change of each day and gets rid of unecessary columns\n",
    "dfStock['DailyChange'] = dfStock['close'] - dfStock['open']\n",
    "dfStock = dfStock.drop(['open', 'close', 'volume', 'high', 'low', 'adj close'], axis=1)\n",
    "dfNews = dfNews.drop(['Stock_symbol', 'Url', 'Publisher', 'Author', 'Article', 'Lsa_summary', 'Luhn_summary', 'Textrank_summary', 'Lexrank_summary'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfE2USHvuoan"
   },
   "outputs": [],
   "source": [
    "#Standardizes the date in order to combine the two dataset\n",
    "dfNews['Date'] = dfNews['Date'].str.split(' ').str[0]\n",
    "dfNews = dfNews.rename(columns={'Date': 'date'})\n",
    "merged_df = pd.merge(dfNews[['date', 'Article_title']], dfStock[['date', 'DailyChange']], on='date', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQ26pdT8vFaR"
   },
   "outputs": [],
   "source": [
    "# Define output path and filename in order to save the combined dataset as a csv file\n",
    "output_dir = \"/media/nickregas/USB OG/CompSciData\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_filename = os.path.join(output_dir, \"DataFile1.csv\")\n",
    "merged_df.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRWXPEPHvX8b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "#Tries to use graphics card but uses cpu if not able to\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda':\n",
    "    print(\"Using device:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"Using device: CPU\")\n",
    "\n",
    "#Defines the specific model for use in the SentenceTransformer library\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "#This method reduces the number of rows in a dataset to a specified fraction while also balancing the amount of dates that a stock increases and decreases to avoid a model always guessing one value\n",
    "def balance_sample(df, sample_frac=0.3):\n",
    "    with tqdm(total=4, desc=\"Balancing Data\") as pbar:\n",
    "        pos = df[df['DailyChange'] > 0]\n",
    "        neg = df[df['DailyChange'] <= 0]\n",
    "        pbar.update(1)\n",
    "\n",
    "        n_samples = int(min(len(pos), len(neg)) * sample_frac)\n",
    "        pos_sampled = pos.sample(n_samples, random_state=42)\n",
    "        neg_sampled = neg.sample(n_samples, random_state=42)\n",
    "        pbar.update(1)\n",
    "\n",
    "        balanced_df = pd.concat([pos_sampled, neg_sampled]).sample(frac=1, random_state=42)\n",
    "        pbar.update(1)\n",
    "\n",
    "        pbar.set_postfix({\"Samples\": len(balanced_df), \"Pos/Neg Ratio\": \"{}/{}\".format(n_samples, n_samples)})\n",
    "        pbar.update(1)\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "balanced_df = balance_sample(merged_df)\n",
    "\n",
    "\n",
    "article_titles = balanced_df['Article_title'].tolist()\n",
    "embeddings = []\n",
    "\n",
    "batch_size = 128\n",
    "#This loop converts the Strings to vectors using the model specified above.\n",
    "with tqdm(total=len(article_titles), desc=\"Generating Embeddings\") as progress_bar:\n",
    "    for start in range(0, len(article_titles), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch = article_titles[start:end]\n",
    "        encoded = model.encode(batch, show_progress_bar=False)\n",
    "        embeddings.append(encoded)\n",
    "        progress_bar.update(len(batch))\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "\n",
    "#This saves the dataset into multiple parquet files which are easier to train a model on.\n",
    "output_path = '/media/nickregas/USB OG/CompSciData/sbert_embeddings.parquet'\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with tqdm(total=3, desc=\"Saving Parquet File\") as pbar:\n",
    "    df_output = pd.DataFrame(\n",
    "        embeddings,\n",
    "        columns=[\"vec_{}\".format(i) for i in range(embeddings.shape[1])]\n",
    "    )\n",
    "    pbar.update(1)\n",
    "\n",
    "    df_output['DailyChange'] = balanced_df['DailyChange'].values\n",
    "    pbar.update(1)\n",
    "\n",
    "    df_output.to_parquet(output_path, index=False)\n",
    "    pbar.update(1)\n",
    "    pbar.set_postfix({\"Path\": output_path})\n",
    "\n",
    "print()\n",
    "print(\"Saved \" + str(len(df_output)) + \" vectors (dim=\" + str(embeddings.shape[1]) + \") to \" + output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dla4IAKByyYT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "print(\"GPU setup complete\")\n",
    "\n",
    "#Retrives data from the parquest files and loads from each one\n",
    "print(\"\\nLoading data...\")\n",
    "parquet_files = sorted(glob.glob('/media/nickregas/USB OG/CompSciData/ParquetFiles/embeddings_*.parquet'))\n",
    "df_list = []\n",
    "for file in tqdm(parquet_files, desc='Loading Files'):\n",
    "    df_list.append(pd.read_parquet(file))\n",
    "df = pd.concat(df_list)\n",
    "\n",
    "#Converts a positive change to increase or True and does the opposite for a negative change\n",
    "X = df.filter(like='vec_')\n",
    "y = (df['DailyChange'] > 0).astype(int)\n",
    "\n",
    "#Splits the data into train and test batches\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nTraining model using GPU...\")\n",
    "#The following class assists in creating a visual progress bar to view how quickly the model is training\n",
    "class ProgressCallback(xgboost.callback.TrainingCallback):\n",
    "    def __init__(self):\n",
    "        self.pbar = None\n",
    "\n",
    "    def before_training(self, model):\n",
    "        self.pbar = tqdm(total=model.num_boosted_rounds(), desc='Training')\n",
    "        return model\n",
    "\n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        self.pbar.update(1)\n",
    "        return False\n",
    "\n",
    "    def after_training(self, model):\n",
    "        self.pbar.close()\n",
    "        return model\n",
    "\n",
    "#Defined the model using the XGBClassifier and with customized parameters\n",
    "model = XGBClassifier(\n",
    "    n_estimators=1500,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.08,\n",
    "    tree_method='gpu_hist',\n",
    "    predictor='gpu_predictor',\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    gamma=1,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=0.3,\n",
    "    min_child_weight=3,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    early_stopping_rounds=40,\n",
    "    callbacks=[ProgressCallback()]\n",
    ")\n",
    "#Training method\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "#Saves model as a JSON layout which allows us to re-use the model without re-training\n",
    "output_dir = '/media/nickregas/USB OG/CompSciData/'\n",
    "model_path = os.path.join(output_dir, 'gpu_stock_model2.json')\n",
    "model.save_model(model_path)\n",
    "print(\"\\nModel saved to \" + model_path)\n",
    "\n",
    "\n",
    "#Generate a matrix to help visualize the models performance\n",
    "plt.figure(figsize=(8, 6))\n",
    "y_pred = model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Predicted Down', 'Predicted Up'],\n",
    "            yticklabels=['Actual Down', 'Actual Up'])\n",
    "\n",
    "plt.title('GPU Stock Prediction Performance')\n",
    "plt.tight_layout()\n",
    "\n",
    "#Saves matrix\n",
    "conf_matrix_path = os.path.join(output_dir, 'gpu_confusion_matrix2.png')\n",
    "plt.savefig(conf_matrix_path, dpi=120)\n",
    "print(\"Confusion matrix image saved to \" + conf_matrix_path)\n",
    "\n",
    "print(\"\\nPipeline finished successfully.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMwGzBa5y1bgdrN6w/Eqi1j",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
